# -*- coding: utf-8 -*-
"""youtube_comments_extracter.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ej2_jtPxAV4uS-RwfifDxzJ0Y1gIMSLn

## Loading Required Libraries
"""

import googleapiclient.discovery
import pandas as pd
import re
from tqdm import tqdm

"""## Dictionary for movies and their youtube links"""

highest_grossing = {'Barbie':'https://www.youtube.com/watch?v=pBk4NYhWNMM',
                    'Oppenheimer':'https://www.youtube.com/watch?v=uYPbbksJxIg',
                    'Guardians of the Galaxy Vol. 3':'https://www.youtube.com/watch?v=u3V5KDHRQvk',
                    'Fast X':'https://www.youtube.com/watch?v=32RAq6JzY-w',
                    'Spider-Man: Across the Spider-Verse':'https://www.youtube.com/watch?v=shW9i6k8cB0',
                    'The Little Mermaid':'https://www.youtube.com/watch?v=kpGo2_d3oYE',
                    'Mission: Impossible – Dead Reckoning Part One':'https://www.youtube.com/watch?v=avz06PDqDbM',
                    'Elemental':'https://www.youtube.com/watch?v=hXzcyx9V0xw',
                    'Ant-Man and the Wasp: Quantumania':'https://www.youtube.com/watch?v=ZlNFpri-Y40'}

lowest_grossing = { 'Amsterdam':'https://www.youtube.com/watch?v=GLs2xxM0e78',
                    'The Last Duel':'https://www.youtube.com/watch?v=mgygUwPJvYk',
                    'West Side Story':'https://www.youtube.com/watch?v=A5GJLwWiYSg',
                    'Mortal Engines':'https://www.youtube.com/watch?v=IRsFc2gguEg',
                    'Pan':'https://www.youtube.com/watch?v=Y1wRv8vTpxo',
                    'Jupiter Ascending':'https://www.youtube.com/watch?v=t4ZzMkDLjWI',
                    'Strange World':'https://www.youtube.com/watch?v=bKh2G73gCCs',
                    'Moonfall':'https://www.youtube.com/watch?v=ivIwdQBlS10',
                    'Cats':'https://www.youtube.com/watch?v=FtSd844cI7U',
                    'Monster Truck':'https://www.youtube.com/watch?v=wQGawWqJdfs'}

average_grossing = { 'Amsterdam':'https://www.youtube.com/watch?v=Euy4Yu6B3nU',
                    '65':'https://www.youtube.com/watch?v=bHXejJq5vr0',
                    'Don’t Worry Darling':'https://www.youtube.com/watch?v=RnvGZUJX6cs',
                    'Black Panther':'https://www.youtube.com/watch?v=_Z3QKkl1WyM',
                    'Chupa':'https://www.youtube.com/watch?v=ViKnrHjzgn4',
                    'A Man Called Otto':'https://www.youtube.com/watch?v=eFYUX9l-m5I',
                    'Sharper':'https://www.youtube.com/watch?v=fKHVHzCGmF0',
                    'The Nun':'https://www.youtube.com/watch?v=QF-oyCwaArU',
                    'Detective Chinatown 2':'https://www.youtube.com/watch?v=0tTiuploPMM',
                    'Venom':'https://www.youtube.com/watch?v=u9Mv98Gr5pY'}

API_key = "AIzaSyCNMGUY4Qgy9rTDgKsaUqk7HtoGlAtrj2g"

"""# Functions"""

def extract_comments(movie_name,video_id,API_key):
  api_service_name = "youtube"
  api_version = "v3"
  DEVELOPER_KEY = API_key

  youtube = googleapiclient.discovery.build(
      api_service_name, api_version, developerKey=DEVELOPER_KEY)

  request = youtube.commentThreads().list(
      part="snippet",
      videoId= video_id,
      maxResults=100
  )

  comments = []

  # Execute the request.
  response = request.execute()

  # Get the comments from the response.
  for item in response['items']:
      comment = item['snippet']['topLevelComment']['snippet']
      public = item['snippet']['isPublic']
      comments.append([
          comment['authorDisplayName'],
          comment['publishedAt'],
          comment['likeCount'],
          comment['textOriginal'],
          public
      ])

  while (1 == 1):
    try:
      nextPageToken = response['nextPageToken']
    except KeyError:
      break
    nextPageToken = response['nextPageToken']
    # Create a new request object with the next page token.
    nextRequest = youtube.commentThreads().list(part="snippet", videoId=video_id, maxResults=100, pageToken=nextPageToken)
    # Execute the next request.
    response = nextRequest.execute()
    # Get the comments from the next response.
    for item in response['items']:
      comment = item['snippet']['topLevelComment']['snippet']
      public = item['snippet']['isPublic']
      comments.append([
          comment['authorDisplayName'],
          comment['publishedAt'],
          comment['likeCount'],
          comment['textOriginal'],
          public
      ])

  df = pd.DataFrame(comments, columns=['author', 'updated_at', 'like_count', 'text','public'])
  df['movie_name'] = movie_name

  return df

def videoID_extracter(video_link):

  pattern = r"\?v=(.*)"

  # Search for the pattern in the string
  match = re.search(pattern, video_link)

  # If there is a match, print the captured group
  if match:
      video_ID = match.group(1)

  return video_ID

def extract_concat_moviesdf(movie_dict):
  movies_df = pd.DataFrame()
  for movie,link in tqdm(movie_dict.items()):
    video_ID = videoID_extracter(link)
    temp_df = extract_comments(movie,video_ID)
    movies_df = pd.concat([movies_df,temp_df])
  return movies_df

"""## Wrapper Function to create DF's"""

movie_category_dict_list = {'highest_grossing_movies':highest_grossing,'lowest_grossing_movies':lowest_grossing,'average_grossing_movies':average_grossing}
for category,category_dict in movie_category_dict_list.items():
  final_df = extract_concat_moviesdf(category_dict)
  final_df.to_pickle(f'{category}.pkl')

"""## **Checking to see if the video has comments**"""

from googleapiclient.discovery import build

def check_comments(video_id, api_key):
    youtube = build('youtube', 'v3', developerKey=api_key)
    request = youtube.commentThreads().list(
        part="snippet",
        videoId=video_id,
        textFormat="plainText"
    )
    response = request.execute()

    if response["items"]:
        return "Yes"
    else:
        return "No"



movie_category_dict_list = [highest_grossing,lowest_grossing,average_grossing]
for category_dict in movie_category_dict_list:
  for movie,link in tqdm(category_dict.items()):
    video_ID = videoID_extracter(link)
    print(check_comments(video_ID, API_key))

"""## data cleaning"""

def clean_data(df, column):
    # This pattern keeps alphanumeric characters (a-z, A-Z, 0-9) and special characters
    pattern = r'[^a-zA-Z0-9!@$%^&,.?\s]'
    df[column] = df[column].apply(lambda x: re.sub(pattern, '', str(x)))

    #removing new line character
    df[column] = df[column].str.replace('\n', ' ')

    # This pattern matches strings that contain only special characters or only numbers
    pattern_2 = r'^[^a-zA-Z]*$'

    # Remove rows where the column's value matches the pattern
    df = df[~df[column].apply(lambda x: bool(re.match(pattern_2, str(x))))]
    return df

df = clean_data(df, 'text')
