# -*- coding: utf-8 -*-
"""Comparing_Algos_for_Sentiment_analysis_YouTube_comments.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12HUoOQ_gWN3lwcibfOGrQAuQ7JZVg9cP

# Importing Required Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification
from scipy.special import softmax
import nltk
nltk.downloader.download('vader_lexicon')
from nltk.sentiment import SentimentIntensityAnalyzer
from tqdm.notebook import tqdm

"""# Reading Data File"""

df = pd.read_pickle('lowest_grossing_movies.pkl')
print(df.shape)

df.movie_name.unique()

"""# Cleaning the Youtube Comments (Text) Column"""

def clean_data(df, column):
    '''
    df: Dataframe containing uncleaned column
    column : Text column that you want to be cleaned
    '''
    # This pattern keeps alphanumeric characters (a-z, A-Z, 0-9) and special characters
    pattern = r'[^a-zA-Z0-9!@$%^&,.?\s]'
    df[column] = df[column].apply(lambda x: re.sub(pattern, '', str(x)))

    #removing new line character
    df[column] = df[column].str.replace('\n', ' ')

    # This pattern matches strings that contain only special characters or only numbers
    pattern_2 = r'^[^a-zA-Z]*$'

    # Remove rows where the column's value matches the pattern
    df = df[~df[column].apply(lambda x: bool(re.match(pattern_2, str(x))))]
    return df

cleaned_movies_df = clean_data(df, 'text')

df=df.reset_index(drop=True)

df.head()

df.shape

"""# Subsetting 700 Comments for each video (Randomly)"""

def subsetting_df(cleaned_movies_df,number_of_comments):
    '''
    cleaned_movies_df : Dataframe containing all the movies and comments
    number_of_comments : Numbers of comments of ewach movie you want to subset
    '''
    movie_df = pd.DataFrame()
    for movie in cleaned_movies_df.movie_name.unique():
        temp_df = df[df['movie_name'] == movie]
        temp_df = temp_df.sample(number_of_comments,replace=True)
        movie_df = pd.concat([movie_df,temp_df])
    return movie_df

df = subsetting_df(cleaned_movies_df,700)

df.movie_name.value_counts()

df = df.reset_index(drop =True).reset_index()

df = df.rename(columns={'index': 'Id'})

df.head()

"""# Using NLTK(Vaders Algo) & Transformer (RoBERTa)"""

MODEL = f"cardiffnlp/twitter-roberta-base-sentiment"
tokenizer = AutoTokenizer.from_pretrained(MODEL)
model = AutoModelForSequenceClassification.from_pretrained(MODEL)

def polarity_scores_roberta(example):
    encoded_text = tokenizer(example, return_tensors='pt')
    output = model(**encoded_text)
    scores = output[0][0].detach().numpy()
    scores = softmax(scores)
    scores_dict = {
        'roberta_neg' : scores[0],
        'roberta_neu' : scores[1],
        'roberta_pos' : scores[2]
    }
    return scores_dict

def sentiment_analyser(df):
    sia = SentimentIntensityAnalyzer()
    res = {}
    for i, row in tqdm(df.iterrows(), total=len(df)):
        try:
            text = row['text']
            myid = row['Id']
            vader_result = sia.polarity_scores(text) # using SIA from NLTK
            vader_result_rename = {}
            for key, value in vader_result.items():
                vader_result_rename[f"vader_{key}"] = value
            roberta_result = polarity_scores_roberta(text) # using roberta from Cardiff transformer
            both = {**vader_result_rename, **roberta_result}
            res[myid] = both
        except RuntimeError:
            print(f'Broke for id {myid}')
    temp_df = pd.DataFrame(res).T
    temp_df = temp_df.reset_index().rename(columns={'index': 'Id'})
    results_df = temp_df.merge(df, how='left')
    return results_df

def vader_sentiment_colADD(vader_df, vader_threshold):
    '''
    vader_df : subset of vader_data fvrom result_df
    vader_threshold : threshold used to segregrate three categories[pos, neg, neu]
    '''
    # Create a mask for positive values
    pos_mask = vader_df['vader_compound'] > vader_threshold

    # Create a mask for negative values
    neg_mask = vader_df['vader_compound'] < -vader_threshold

    # Assign 'positive' to the rows where pos_mask is True
    vader_df.loc[pos_mask, 'vader_sentiment'] = 'positive'

    # Assign 'negative' to the rows where neg_mask is True
    vader_df.loc[neg_mask, 'vader_sentiment'] = 'negative'

    # Assign 'neutral' to the remaining rows
    vader_df.loc[~(pos_mask | neg_mask), 'vader_sentiment'] = 'neutral'

    return vader_df

def roberta_sentiment_colADD(roberta_df):
    '''
    roberta_df : subset of roberta_data from result_df
    '''
    max_column = roberta_df[['roberta_neg', 'roberta_neu', 'roberta_pos']].idxmax(axis=1)

    roberta_df['Max_Column'] = max_column

    # Create a mask for negative values
    neg_mask = roberta_df['Max_Column'] == 'roberta_neg'

    # Create a mask for neutral values
    neu_mask = roberta_df['Max_Column'] == 'roberta_neu'

    # Assign 'negative' to the rows where neg_mask is True
    roberta_df.loc[neg_mask, 'roberta_sentiment'] = 'negative'

    # Assign 'neutral' to the rows where neu_mask is True
    roberta_df.loc[neu_mask, 'roberta_sentiment'] = 'neutral'

    # Assign 'positive' to the remaining rows
    roberta_df.loc[~(neg_mask | neu_mask), 'roberta_sentiment'] = 'positive'

    return roberta_df

def algo_result(algo_df,algo_name):
    '''
    algo_df : df after adding the sentiment column to any of the algo
    algo_name : 'roberta' OR 'vader' - can contain value only from this
    '''
    m_names = vader_df['movie_name'].unique()
    algo_dic={}
    for i in m_names:
        tem_algo_df = algo_df[algo_df['movie_name'] == i]
        mode = tem_algo_df[f'{algo_name}_sentiment'].mode()[0]
        algo_dic[i]=mode

    algo_result = pd.DataFrame(algo_dic,index=[0]).T.reset_index().rename(columns={0: "Sentiment",'index' : 'Movie'})
    return algo_result

results_df = sentiment_analyser(df)

vader_df=results_df[['movie_name','text','vader_neg','vader_neu','vader_pos','vader_compound']]
vader_df = vader_sentiment_colADD(vader_df, 0.1)

roberta_df=results_df[['movie_name','text','roberta_neg','roberta_neu','roberta_pos']]

roberta_df = roberta_sentiment_colADD(roberta_df)

"""# Results from Vader (NLTK)"""

vader_movie_result = algo_result(vader_df,'vader')
vader_movie_result

"""# Results from Roberta (CardiffNLP) - Using Transformer"""

roberta_movie_result = algo_result(roberta_df,'roberta')
roberta_movie_result