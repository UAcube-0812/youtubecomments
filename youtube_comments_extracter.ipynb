{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZwnOi8E6J3gWTHilujyXD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aviralwalia08/youtubecomments/blob/main/youtube_comments_extracter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Required Libraries"
      ],
      "metadata": {
        "id": "wazk2E3L3dxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import googleapiclient.discovery\n",
        "import pandas as pd\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n"
      ],
      "metadata": {
        "id": "HLrjdFwSD7uY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dictionary for movies and their youtube links"
      ],
      "metadata": {
        "id": "kDePj1qP4NiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "highest_grossing = {'Barbie':'https://www.youtube.com/watch?v=pBk4NYhWNMM',\n",
        "                    'Oppenheimer':'https://www.youtube.com/watch?v=uYPbbksJxIg',\n",
        "                    'Guardians of the Galaxy Vol. 3':'https://www.youtube.com/watch?v=u3V5KDHRQvk',\n",
        "                    'Fast X':'https://www.youtube.com/watch?v=32RAq6JzY-w',\n",
        "                    'Spider-Man: Across the Spider-Verse':'https://www.youtube.com/watch?v=shW9i6k8cB0',\n",
        "                    'The Little Mermaid':'https://www.youtube.com/watch?v=kpGo2_d3oYE',\n",
        "                    'Mission: Impossible – Dead Reckoning Part One':'https://www.youtube.com/watch?v=avz06PDqDbM',\n",
        "                    'Elemental':'https://www.youtube.com/watch?v=hXzcyx9V0xw',\n",
        "                    'Ant-Man and the Wasp: Quantumania':'https://www.youtube.com/watch?v=ZlNFpri-Y40'}\n",
        "\n",
        "lowest_grossing = { 'Amsterdam':'https://www.youtube.com/watch?v=GLs2xxM0e78',\n",
        "                    'The Last Duel':'https://www.youtube.com/watch?v=mgygUwPJvYk',\n",
        "                    'West Side Story':'https://www.youtube.com/watch?v=A5GJLwWiYSg',\n",
        "                    'Mortal Engines':'https://www.youtube.com/watch?v=IRsFc2gguEg',\n",
        "                    'Pan':'https://www.youtube.com/watch?v=Y1wRv8vTpxo',\n",
        "                    'Jupiter Ascending':'https://www.youtube.com/watch?v=t4ZzMkDLjWI',\n",
        "                    'Strange World':'https://www.youtube.com/watch?v=bKh2G73gCCs',\n",
        "                    'Moonfall':'https://www.youtube.com/watch?v=ivIwdQBlS10',\n",
        "                    'Cats':'https://www.youtube.com/watch?v=FtSd844cI7U',\n",
        "                    'Monster Truck':'https://www.youtube.com/watch?v=wQGawWqJdfs'}\n",
        "\n",
        "average_grossing = { 'Amsterdam':'https://www.youtube.com/watch?v=Euy4Yu6B3nU',\n",
        "                    '65':'https://www.youtube.com/watch?v=bHXejJq5vr0',\n",
        "                    'Don’t Worry Darling':'https://www.youtube.com/watch?v=RnvGZUJX6cs',\n",
        "                    'Black Panther':'https://www.youtube.com/watch?v=_Z3QKkl1WyM',\n",
        "                    'Chupa':'https://www.youtube.com/watch?v=ViKnrHjzgn4',\n",
        "                    'A Man Called Otto':'https://www.youtube.com/watch?v=eFYUX9l-m5I',\n",
        "                    'Sharper':'https://www.youtube.com/watch?v=fKHVHzCGmF0',\n",
        "                    'The Nun':'https://www.youtube.com/watch?v=QF-oyCwaArU',\n",
        "                    'Detective Chinatown 2':'https://www.youtube.com/watch?v=0tTiuploPMM',\n",
        "                    'Venom':'https://www.youtube.com/watch?v=u9Mv98Gr5pY'}\n",
        "\n",
        "API_key = \"AIzaSyCNMGUY4Qgy9rTDgKsaUqk7HtoGlAtrj2g\"\n"
      ],
      "metadata": {
        "id": "TUZ3T_Sac4cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "QVJ9GNmf4Zfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_comments(movie_name,video_id,API_key):\n",
        "  api_service_name = \"youtube\"\n",
        "  api_version = \"v3\"\n",
        "  DEVELOPER_KEY = API_key\n",
        "\n",
        "  youtube = googleapiclient.discovery.build(\n",
        "      api_service_name, api_version, developerKey=DEVELOPER_KEY)\n",
        "\n",
        "  request = youtube.commentThreads().list(\n",
        "      part=\"snippet\",\n",
        "      videoId= video_id,\n",
        "      maxResults=100\n",
        "  )\n",
        "\n",
        "  comments = []\n",
        "\n",
        "  # Execute the request.\n",
        "  response = request.execute()\n",
        "\n",
        "  # Get the comments from the response.\n",
        "  for item in response['items']:\n",
        "      comment = item['snippet']['topLevelComment']['snippet']\n",
        "      public = item['snippet']['isPublic']\n",
        "      comments.append([\n",
        "          comment['authorDisplayName'],\n",
        "          comment['publishedAt'],\n",
        "          comment['likeCount'],\n",
        "          comment['textOriginal'],\n",
        "          public\n",
        "      ])\n",
        "\n",
        "  while (1 == 1):\n",
        "    try:\n",
        "      nextPageToken = response['nextPageToken']\n",
        "    except KeyError:\n",
        "      break\n",
        "    nextPageToken = response['nextPageToken']\n",
        "    # Create a new request object with the next page token.\n",
        "    nextRequest = youtube.commentThreads().list(part=\"snippet\", videoId=video_id, maxResults=100, pageToken=nextPageToken)\n",
        "    # Execute the next request.\n",
        "    response = nextRequest.execute()\n",
        "    # Get the comments from the next response.\n",
        "    for item in response['items']:\n",
        "      comment = item['snippet']['topLevelComment']['snippet']\n",
        "      public = item['snippet']['isPublic']\n",
        "      comments.append([\n",
        "          comment['authorDisplayName'],\n",
        "          comment['publishedAt'],\n",
        "          comment['likeCount'],\n",
        "          comment['textOriginal'],\n",
        "          public\n",
        "      ])\n",
        "\n",
        "  df = pd.DataFrame(comments, columns=['author', 'updated_at', 'like_count', 'text','public'])\n",
        "  df['movie_name'] = movie_name\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "xcDqSn3uhp2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def videoID_extracter(video_link):\n",
        "\n",
        "  pattern = r\"\\?v=(.*)\"\n",
        "\n",
        "  # Search for the pattern in the string\n",
        "  match = re.search(pattern, video_link)\n",
        "\n",
        "  # If there is a match, print the captured group\n",
        "  if match:\n",
        "      video_ID = match.group(1)\n",
        "\n",
        "  return video_ID"
      ],
      "metadata": {
        "id": "9N_cAMBwkJdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_concat_moviesdf(movie_dict):\n",
        "  movies_df = pd.DataFrame()\n",
        "  for movie,link in tqdm(movie_dict.items()):\n",
        "    video_ID = videoID_extracter(link)\n",
        "    temp_df = extract_comments(movie,video_ID)\n",
        "    movies_df = pd.concat([movies_df,temp_df])\n",
        "  return movies_df\n",
        "\n"
      ],
      "metadata": {
        "id": "ZCMuBs4YGpv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wrapper Function to create DF's"
      ],
      "metadata": {
        "id": "KpShKQbM7NaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "movie_category_dict_list = {'highest_grossing_movies':highest_grossing,'lowest_grossing_movies':lowest_grossing,'average_grossing_movies':average_grossing}\n",
        "for category,category_dict in movie_category_dict_list.items():\n",
        "  final_df = extract_concat_moviesdf(category_dict)\n",
        "  final_df.to_pickle(f'{category}.pkl')\n",
        "\n"
      ],
      "metadata": {
        "id": "6wiQjM4vti3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Checking to see if the video has comments**"
      ],
      "metadata": {
        "id": "XFwR3ZKeg0Wv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "\n",
        "def check_comments(video_id, api_key):\n",
        "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
        "    request = youtube.commentThreads().list(\n",
        "        part=\"snippet\",\n",
        "        videoId=video_id,\n",
        "        textFormat=\"plainText\"\n",
        "    )\n",
        "    response = request.execute()\n",
        "\n",
        "    if response[\"items\"]:\n",
        "        return \"Yes\"\n",
        "    else:\n",
        "        return \"No\""
      ],
      "metadata": {
        "id": "BXxHKDhqcUX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bBHfqDrOOvxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movie_category_dict_list = [highest_grossing,lowest_grossing,average_grossing]\n",
        "for category_dict in movie_category_dict_list:\n",
        "  for movie,link in tqdm(category_dict.items()):\n",
        "    video_ID = videoID_extracter(link)\n",
        "    print(check_comments(video_ID, API_key))"
      ],
      "metadata": {
        "id": "oWPezYrJfdxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## data cleaning"
      ],
      "metadata": {
        "id": "HzYxkSp9N0pP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data(df, column):\n",
        "    # This pattern keeps alphanumeric characters (a-z, A-Z, 0-9) and special characters\n",
        "    pattern = r'[^a-zA-Z0-9!@$%^&,.?\\s]'\n",
        "    df[column] = df[column].apply(lambda x: re.sub(pattern, '', str(x)))\n",
        "\n",
        "    #removing new line character\n",
        "    df[column] = df[column].str.replace('\\n', ' ')\n",
        "\n",
        "    # This pattern matches strings that contain only special characters or only numbers\n",
        "    pattern_2 = r'^[^a-zA-Z]*$'\n",
        "\n",
        "    # Remove rows where the column's value matches the pattern\n",
        "    df = df[~df[column].apply(lambda x: bool(re.match(pattern_2, str(x))))]\n",
        "    return df"
      ],
      "metadata": {
        "id": "hsazapeSN2Lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = clean_data(df, 'text')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "agNE4_pVOOxu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}